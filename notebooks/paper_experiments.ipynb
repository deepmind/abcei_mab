{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARfv6pGIP7mS"
      },
      "source": [
        "Copyright 2022 DeepMind Technologies Limited. All Rights Reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VdkxvxoPhBB"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 DeepMind Technologies Limited\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ7cXYnsD8mQ"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook introduces the code framework for reproducing the results of the NeurIPS paper,\n",
        "\n",
        "Malek, Alan, and Silvia Chiappa. \"Asymptotically Best Causal Effect Identification with Multi-Armed ceb.bandits.\" Advances in Neural Information Processing Systems 34 (2021).\n",
        "\n",
        "We will introduce the code stucture by way of a few examples before providing the code that generated plots in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BqgvDdsotHc"
      },
      "source": [
        "## Implementation details \n",
        "We begin by going through the various classes and what abstractions they reperesent. The main package is `causal_effect_bandits`, which we will import as `ceb` throughout.\n",
        "\n",
        "### Passing Data\n",
        "Observational data is stored in the `ceb.data.ExpData` class, which keeps numpy arrays for covariates $Z$, exposure $X$, and response $Y$. For data `d`, they can be accessed by `d.cov`, `d.exp`, and `d.rsp`, respectively. Throughout this notebook, we will always use $Z$ for covariates, $X$ for exposure, and $Y$ for response.\n",
        "\n",
        "### Generating Observational Data\n",
        "The `data` module also has a `ceb.data.DataGenerator` class with a `.generate(n)` method that will produce an `ceb.data.ExpData` object of `n` samples. The most common data generating process we will use is a Structural Causal Model (`ceb.data.SCM`), which is implemented in the `SCM` module.\n",
        "\n",
        "### Modelling an Estimator\n",
        "Our estimators are specified by an uncentered influence function $\\phi$ and a nuisance parameter $\\eta$; these two objects are implemented by the `ceb.parameters.NuisanceParameter` class, where each instance has the following components:\n",
        "- object `self._model`, representing $\\eta$, that we may `.fit` and `.transform` (with the usual sklearn AIP)\n",
        "- method `fit(W: ceb.data.ExpData)` that uses observation data $W$ to fit $\\hat\\eta(W)$\n",
        "- method `_phi(eta: np.ndarray,  W: ceb.data.ExpData)` that  return a vector $(\\phi(w_i, \\eta_i): \\eta_i \\in \\text{eta}, w_i \\in \\text{W})$\n",
        "- method `calculate_score(W: ceb.data.ExpData)` function that first calculates $\\eta$ from `W then returns the array $(\\phi(w, \\hat\\eta(w)): w \\in \\text{W})$, on the fit $\\hat\\eta$.\n",
        "- method `calculate_cs_width(delta, args*)`: calculates a confidence interval width on the error of the nuissanc parameter, $\\Vert \\hat\\eta - \\eta\\Vert$. This error is needed by the confidence intervals for the asymptotic variance estimator.\n",
        "\n",
        "Specific examples of eta (e.g. the AIPW) will be implemented by subclassing `ceb.parameters.NuissanceParameter`, including the following classes\n",
        "  - `ceb.parameters.LinearParameter`: fits the two conditional response function, $\\mu(1, z)$ and $\\mu(0, z)$, using linear regression\n",
        "  -`ceb.parameters.AIPW`: fits the augmented inverse propensity weight estimator by fitting the conditional response $\\mathbb E[Y|Z,X]$ and the propensity score $e(z) = p(X=1|Z=z)$.\n",
        "  - `ceb.parameters.FrontdoorLinearFiniteX`: implements the frontdoor formula described in the paper appendix.\n",
        "\n",
        "\n",
        "### Estimating the Asymptotic Variance\n",
        "The `ceb.arms.VarianceEstimatorArm` represents an estimator of the causal effect $\\tau_k$, and contains code to build a confidence sequence on an estimate of the asymptotic variance of the estimator $\\sigma_k^2$. This class has the following attributes and methods.\n",
        "* object `self._eta`, a `parameter.NuisanceParameter` instance.\n",
        "* method `update(new_data: ceb.data.ExpData, delta: float)`: updates the confidence sequences and parameter estimates using data `new_data` such that the total error of the confidence sequence is at most `delta`.\n",
        "* method `reset()`: resets the arms to its initial value, including the parameter estimate in `self._eta`.\n",
        "* properties `ci`, `ate`, and `var_est`, which hold the current confidence interval, estimate of $\\tau_k$, and estimate of $\\sigma^2_k$, respectively.\n",
        "* attribute `self._data_transformer: Callable[[ceb.data.ExpData], ceb.data.ExpData]`, a function. This takes an `ceb.data.ExpData` object of observational data (e.g. generated by a `ceb.data.DataGenerator` object) and transforms it into an `ceb.data.ExpData` object that is expected by `self._eta`. Examples of these functions are returned by \n",
        " * `ceb.data.get_identity_fn()` (which returns the identity function),\n",
        " * `ceb.data.get_remove_coordinate_fn(idx)` which returns a function that maps an `ceb.data.ExpData` object into an `ceb.data.ExpData` object with the same `.exp` and `.rsp` and the `.cov` with the coordinates in idx removed.\n",
        " * `ceb.data.get_coordinate_mask_fn(idx)` which returns a function that maps an `ceb.data.ExpData` object into an `ceb.data.ExpData` object with the same `.exp` and `.rsp` and the `.cov` with only the coordinates in idx remaining.\n",
        "\n",
        "\n",
        "The most common example is if an `ceb.data.ExpData` object `d` contains more covariates in `d.cov` than is needed by the `VarianceEstimatorArm`. Suppose `d` has `d.cov` with two dimensions, the first corresponding to $Z_1$ and the second corresponding to $Z_2$. Then creating an `arm.VarianceEstimatorArm` with `_data_transformer = ceb.data.get_remove_coordinate_fn(1)` corresponds to an arm that only sees $Z_1$, and creating an `arm.VarianceEstimatorArm` with `_data_transformer = ceb.data.get_remove_coordinate_fn(0)` corresponds to an arm that only sees $Z_2$.\n",
        "\n",
        "### Running a Bandit Algorithm over Arms\n",
        "A `ceb.bandits.BanditAlgorithm` class holds a collection of `ceb.arms.VarianceEstimatorArm` and a `ceb.data.DataGenerator` and implements a specific bandit algorithm (which specifies which arm to pull during the next round) and generates data from the `ceb.data.DataGenerator` object. Important attributes and methods for this class include the following. \n",
        "\n",
        "The only important method is `self.run(all_data: Optional[ceb.data.ExpData])`, which runs the bandit algorithm either on the provided dataset, or on the `ceb.data.DataGenerator` provided. Specifying a static dataset makes comparing the performances of various bandit algorithms have lower variance. This method returns a `ceb.bandits.BanditData`object, which is a `dataclass` that contains:\n",
        "* `lcb_by_arm`  a dictionary with `arm.VarianceEstimatorArm` keys that collects an array of all the the lower confidence bound of that arm for every iteration of the bandit algorithm\n",
        "* `ucb_by_arm` an analogous dictionary of arrays for the upper confidence bounds.\n",
        "* `var_est_by_arm` an analogous dictionary of arrays for the variance estimates.\n",
        "* `samples_by_arm` an analogous dictionary of arrays for the number of new samples used (it is not cumulative).\n",
        "* `cum_samples`: an array of the cumulative number of samples used by every arm for every iteration of the bandit algorithm\n",
        "* `best_arms`: a list of the best arm(s).\n",
        "\n",
        "Bandit algorithms implemented include:\n",
        "* `UniformAlgorithm`: samples every arm every round\n",
        "* `LUCBAlgorithm`: implements the LUCB algorithm from\n",
        "``` Kalyanakrishnan, Shivaram, et al. \"PAC subset selection in stochastic multi-armed ceb.bandits.\" ICML. Vol. 12. 2012.```\n",
        "\n",
        "* 'SuccessiveEliminationAlgorithm`: implements the successive elimination algorithm from \n",
        "```Even-Dar, Eyal, et al. \"Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems.\" Journal of machine learning research 7.6 (2006).```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAZqx23R_GY"
      },
      "source": [
        "## Setup and Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8jmILqYSEQm"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/deepmind/abcei_mab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHsd1stqqjQ-"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import causal_effect_bandits as ceb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fH4NjUYH3gO"
      },
      "source": [
        "## Minimum Working Example\n",
        "With all that out of the way, let us start with a simple SCM that has three nodes, Z, X, and Y. The edges in this graph are $X\\leftarrow Z\\rightarrow Y$ and $X\\rightarrow Y$; that is, $Z$ confounds $X$ and $Y$ so \n",
        "we cannot get an unbiased estimate for the treatment effect if we ignore $Z$.\n",
        "\n",
        "We also include an irrelevent variable $Z_2$, statistically independent from the other variables (i.e. with no parents or children) to \n",
        "illustrate how to use data_transformer to ignore it.\n",
        "\n",
        "We now specify the conditional probability distributions of each node\n",
        "by building a dictionary. We will define\n",
        "- $Z \\sim$ Bernoulli$(1/3)$ (it has no parents), \n",
        "- $T|Z=z \\sim$ Bernoulli$(1/2 + .1z)$,\n",
        "- $Y|X=x,Z=z \\sim$ Normal$(1/2 + x/4 - z/4, 0)$, and\n",
        "- $Z_2 \\sim$ Normal$(0)$\n",
        "\n",
        "Each cpd is a function mapping from n (a sample size) and parents, a \n",
        "List[np.ndarray], where each entry in the list is of size(n, parent_dimension), \n",
        "and the length of the list is equal to the number of parents of the node.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IoVm5v5JmUz"
      },
      "outputs": [],
      "source": [
        "var_names = [\"Z\", \"Z2\", \"X\", \"Y\"]\n",
        "cov_variables = [\"Z\", \"Z2\"]\n",
        "treatment_variable = \"X\"\n",
        "response_variable = \"Y\"\n",
        "\n",
        "# We specify the graph by listing the parents of each node.\n",
        "parents = {\n",
        "    \"Z\": [],\n",
        "    \"X\": [\"Z\"],\n",
        "    \"Y\": [\"Z2\", \"X\"],\n",
        "    \"Z2\": [\"Z\"],\n",
        "}\n",
        "\n",
        "cpds = {\n",
        "    \"Z\":\n",
        "        lambda n, parents: np.random.binomial(1, 1/3, size=n),\n",
        "    \"X\":\n",
        "        lambda n, parents: np.random.binomial(1, .5 + .1 * parents[0]),\n",
        "    \"Z2\":\n",
        "        lambda n, parents: np.random.normal(.1 * parents[0], 1),\n",
        "    \"Y\":      \n",
        "        lambda n, parents: np.random.normal(size=len(parents[0])) + .25 *\n",
        "          parents[1] + .25 * parents[0],\n",
        "}\n",
        "\n",
        "# We can now call the SCM constructor and generate data\n",
        "scm_gen = ceb.scm.SCM(\n",
        "    name=\"back-door example\",\n",
        "    var_names=var_names,\n",
        "    parents=parents,\n",
        "    cpds=cpds,\n",
        "    cov_variables=cov_variables,\n",
        "    treatment_variable=treatment_variable,\n",
        "    response_variable=response_variable,\n",
        ")\n",
        "\n",
        "# Finally, we generate 10 datapoints and print the ExpData object.\n",
        "# The first column of Z corresponds to Z, which is binary, and the second \n",
        "# column to Z2, which is continuous.\n",
        "\n",
        "scm_gen.generate(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmZiZqqQRvtJ"
      },
      "source": [
        "_Expected output_:\n",
        "A 2-dimensional covariates matrix, with the first collumn being binary, along with an exposure and response vectors, all of length 10.\n",
        "\n",
        "The next step is to define two VarianceEstimatorArms using the back-door formula a.k.a. covariate adjustment. One arm will just use $Z$ and one will use $Z$ and $Z_2$. \n",
        "\n",
        "We need to specify the scale of the problem, which we do by estimating the variance on a small dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgA3pk7GOjcn"
      },
      "outputs": [],
      "source": [
        "d = scm_gen.generate(1000)\n",
        "remove_Z2 = ceb.data.get_remove_coordinates_fn(1)\n",
        "remove_Z = ceb.data.get_remove_coordinates_fn(0)\n",
        "Z_var = np.var(remove_Z2(d).cov)\n",
        "ZZ2_var = np.var(d.cov)\n",
        "\n",
        "armZ = ceb.arms.SampleSplittingArm(\n",
        "      'Z_adjustment',\n",
        "      eta=ceb.parameters.AIPWLinearLogistic(n_features=1),\n",
        "      # remove the Z2 coordinate\n",
        "      data_transformer=remove_Z2,\n",
        "      ci_algorithm=ceb.arms.CIAlgorithm.CLT,\n",
        "      sub_gscale=Z_var,\n",
        "      tau_bound=2,\n",
        "      burn_in=100,\n",
        "      rho=1,\n",
        ")\n",
        "armZZ2 = ceb.arms.SampleSplittingArm(\n",
        "      'Z2_adjustment',\n",
        "      eta=ceb.parameters.AIPWLinearLogistic(n_features=1),\n",
        "      # do not remove the Z2 coordinate\n",
        "      data_transformer=remove_Z,\n",
        "      ci_algorithm=ceb.arms.CIAlgorithm.CLT,\n",
        "      sub_gscale=ZZ2_var,\n",
        "      tau_bound=2,\n",
        "      burn_in=100,\n",
        "      rho=1,\n",
        ")\n",
        "\n",
        "# Next, we define the bandit instance. \n",
        "bandit = ceb.bandits.UniformAlgorithm(\n",
        "    arm_list=[armZ, armZZ2],\n",
        "    prob_model=scm_gen,\n",
        "    confidence=.05,\n",
        "    sample_limit=150000,\n",
        "    units_per_round=1000,\n",
        ")\n",
        "\n",
        "# We run the bandit algorithm to get bandit data\n",
        "np.random.seed(0)\n",
        "bdata = bandit.run()\n",
        "print(f\"The best arm was found to be {bdata.best_arm[0].name}\")\n",
        "\n",
        "# and plot the results\n",
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "ceb.plotting_utils.plot_bandit_results(ax, [armZ, armZZ2], bdata, 1)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSbhRp2GTf3G"
      },
      "source": [
        "_Expected Output_:\n",
        "We should see the confidence sequencences gradually tighten until the blue interval, which corresponds to adjusting by $Z$, stops intersecting the orange interval. At this point, the algorithm terminates and outputs that adjusting by $Z_2$ is superior, which agrees with the theory that predicts that adjusting by variables closer to the response $Y$ results in a lower variance estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOWzlQBvfZEb"
      },
      "source": [
        "# Plots from the Paper\n",
        "\n",
        "Next, we turn towards generating the plots from the paper. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qblQVhaUbwww"
      },
      "source": [
        "## Experiments from Section 5.1\n",
        "\n",
        "The larger the number of possible adjustment sets, the bigger the potential sample savings a bandit algortihm can provide.\n",
        "\n",
        "We will generalize the back door and front door example from the section. Instead of there being two back door paths $X\\leftarrow V_i \\rightarrow Z_i\\rightarrow Y$, for $i=1,2$, we will consider $K$ back-door paths. Any valid adjustment set will have to block all of them, and so it must include either $V_i$ or $Z_i$ for all $i=1,\\ldots, K$; therefore, there are $2^K$ valid adjustment sets.\n",
        "\n",
        "In addition, we have a possible frontdoor adjustment path from $X\\rightarrow Z_M\\rightarrow Y$.\n",
        "\n",
        "On a side note, we can include adjustment by the front and back door by using the expression derived in example 10 of Smucler and Rotnitzky, 2019. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3JNf_63ucEE"
      },
      "outputs": [],
      "source": [
        "# First, we define the numer of back-door paths and the dimension\n",
        "num_back_door_paths = 3\n",
        "sample_limit = 200000\n",
        "units_per_round = 500\n",
        "z_dim = [2] + [3 for _ in range(num_back_door_paths)]  # include Z0\n",
        "v_dim = [0] + [2 for _ in range(num_back_door_paths)]  # include space for V0\n",
        "\n",
        "# We generate the SCM, VarianceEstimatorArms, and the bandit algorithms\n",
        "args = ceb.paper_utils.get_section_5_1_example(z_dim, \n",
        "                               v_dim,\n",
        "                               seed=2,\n",
        "                               z_cost=3,\n",
        "                               v_cost=1,\n",
        "                               z0_cost=3,\n",
        "                               num_back_door_paths=3,\n",
        "                               sample_limit = sample_limit,\n",
        "                               units_per_round = units_per_round,\n",
        "                               latex_names = True,\n",
        ")\n",
        "(scm_gen, arm_list, LUCB_bandit, SE_bandit, uniform_bandit) = args\n",
        "\n",
        "# To help the comparison between the arms, we fix the the data\n",
        "#all_data = scm_gen.generate(sample_limit + units_per_round * len(arm_list))\n",
        "\n",
        "results = SE_bandit.run()\n",
        "print(f\"The best arm was found to be {results.best_arm[0].name} after {max(results.cum_samples)} samples were used.\")\n",
        "\n",
        "# Plot the results of the best m arms\n",
        "m = 4\n",
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "plt.ticklabel_format(axis='x', style='scientific',scilimits=(0,0))\n",
        "cutoff_var = np.sort([results.var_est_by_arm[a][-1] for a in arm_list])[m]\n",
        "top_arms = [a for a in arm_list if results.var_est_by_arm[a][-1] \u003c cutoff_var]\n",
        "ceb.plotting_utils.plot_bandit_results(ax, top_arms, results, initial_points_to_ignore=4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VK4gWygUzwz"
      },
      "source": [
        "_Expected Output_: the frontdoor arm, in blue, should be the best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trx0t2yAy8S_"
      },
      "source": [
        "## Section 5.1, continued\n",
        "\n",
        "Next, we generate Figure 1, b), which requires that we run the three bandit algorithms on multiple instances and collect the sample complexities.\n",
        "\n",
        "There is a lot of simulation, so this cell can take around an hour to finish running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnimTMLP3m2F"
      },
      "outputs": [],
      "source": [
        "n_instances = 10\n",
        "n_repetitions = 5\n",
        "num_back_door_paths = 3\n",
        "sample_limit = 200000\n",
        "units_per_round = 1000\n",
        "Z_dim = [2] + [3 for _ in range(num_back_door_paths)]  # include Z0\n",
        "V_dim = [0] + [2 for _ in range(num_back_door_paths)]  # include space for V0\n",
        "\n",
        "LUCB_samples = []\n",
        "SE_samples = []\n",
        "uniform_bandit_samples = []\n",
        "instance_history = []\n",
        "\n",
        "instance = -1\n",
        "while len(LUCB_samples) \u003c n_instances * n_repetitions:\n",
        "  instance += 1\n",
        "  print(f\"At instance {instance}\")\n",
        "  # Approximate the variances to make sure the instance isn't too hard.\n",
        "  # We generate the SCM, VarianceEstimatorArms, and the bandit algorithms\n",
        "  args = ceb.paper_utils.get_section_5_1_example(\n",
        "      Z_dim,\n",
        "      V_dim,\n",
        "      seed=instance,\n",
        "      z_cost=2,\n",
        "      v_cost=1,\n",
        "      z0_cost=5,\n",
        "      num_back_door_paths=3,\n",
        "      sample_limit=sample_limit,\n",
        "      units_per_round=units_per_round,\n",
        "  )\n",
        "  (scm_gen, arm_list, LUCB_bandit, SE_bandit, uniform_bandit) = args\n",
        "\n",
        "  temp_data = scm_gen.generate(10000)\n",
        "  var_hat_tau = []\n",
        "  for arm in arm_list:\n",
        "    arm.reset()\n",
        "    arm_data = arm._data_transformer(temp_data).k_fold(2)\n",
        "    arm._eta.fit(arm_data[0])\n",
        "    var_hat_tau.append(arm.cost * np.var(arm._eta.calculate_score(arm_data[1])))\n",
        "\n",
        "  sorted_vars = np.array(var_hat_tau)\n",
        "  sorted_vars.sort()\n",
        "  min_gap = sorted_vars[1] - sorted_vars[0]\n",
        "\n",
        "  if min_gap \u003e 5:\n",
        "    print(f\"The minimum gap is {min_gap}. Proceeding!\")\n",
        "  else:\n",
        "    print(f\"The minimum gap is {min_gap}. Generating a new instance!\")\n",
        "    continue\n",
        "\n",
        "  for m in range(n_repetitions):\n",
        "    instance_history.append(instance)\n",
        "    all_data = scm_gen.generate(sample_limit + units_per_round * len(arm_list))\n",
        "\n",
        "    # Run LUCB\n",
        "    LUCB_bandit.reset()\n",
        "    np.random.seed(m)\n",
        "    LUCB_results = LUCB_bandit.run(all_data)\n",
        "    LUCB_samples.append(LUCB_results.cum_samples[-1])\n",
        "\n",
        "    # Run SuccessiveElimination\n",
        "    SE_bandit.reset()\n",
        "    np.random.seed(m)\n",
        "    SE_results = SE_bandit.run(all_data)\n",
        "    SE_samples.append(SE_results.cum_samples[-1])\n",
        "    \n",
        "    # Calculate the number of samples needed by uniform_bandit\n",
        "    total_samples_by_arm = [sum(SE_results.samples_by_arm[a]) for a in arm_list]\n",
        "    max_samples = max(total_samples_by_arm)\n",
        "    # uniform_bandit would use max_samples for every arm\n",
        "    uniform_bandit_samples.append(max_samples * len(arm_list))\n",
        "    \n",
        "    print((f'instance {instance} had samples: \\tLUCB:{LUCB_samples[-1]}\\t'),\n",
        "      (f'SE:{SE_samples[-1]}\\tuniform:{uniform_bandit_samples[-1]}'))\n",
        "  \n",
        "fig, ax = plt.subplots()\n",
        "plt.boxplot([LUCB_samples, SE_samples, uniform_bandit_samples],\n",
        "            labels=['$CS-LUCB$', '$CS-SE$', '$Uniform$'])\n",
        "plt.ticklabel_format(axis='y', style='scientific',scilimits=(0,0))\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROOWumu1WuoF"
      },
      "source": [
        "_Expected Output_: The scrips iterates through instances (random seeds) until it finds a SCM with a large enough gap between the two ceb.arms. Then, it prints the number of samples needed by LUCB, SE, and the uniform algorithm for 5 different random datasets from this instance. The script them moves onto to a different random SCM. After all this is finished (which may take some time), we should see a box plot indicating that both LUCB and SE have substantially smaller sample complexities than the uniform algorithm, with SE being slightly better than LUCB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UcFi2AmrrIR"
      },
      "source": [
        "## Experiments from Section 5.2\n",
        "\n",
        "The causal graph is the same as the previous section, except all the functional relationships are nonlinear and sampled from a Gaussian Process. \n",
        "\n",
        "There is a lot of simulation, so expect potentially a few hours for this cell to finish running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k_aoPVdIkjx"
      },
      "outputs": [],
      "source": [
        "n_instances = 10\n",
        "n_repetitions = 5\n",
        "num_back_door_paths = 3\n",
        "sample_limit = 500000\n",
        "units_per_round = 5000\n",
        "Z_dim = [2] + [3 for _ in range(num_back_door_paths)]  # include Z0\n",
        "V_dim = [0] + [2 for _ in range(num_back_door_paths)]  # include space for V0\n",
        "\n",
        "LUCB_samples = []\n",
        "SE_samples = []\n",
        "uniform_bandit_samples = []\n",
        "instance_history = []\n",
        "\n",
        "instance = -1\n",
        "while len(LUCB_samples) \u003c n_instances * n_repetitions:\n",
        "  instance += 1\n",
        "  print(f\"At instance {instance}\")\n",
        "  # Approximate the variances to make sure the instance isn't too hard.\n",
        "  # We generate the SCM, VarianceEstimatorArms, and the bandit algorithms\n",
        "  args = ceb.paper_utils.get_section_5_2_example(\n",
        "      Z_dim,\n",
        "      V_dim,\n",
        "      seed=instance,\n",
        "      z_cost=2,\n",
        "      v_cost=1,\n",
        "      z0_cost=5,\n",
        "      num_back_door_paths=3,\n",
        "      sample_limit=sample_limit,\n",
        "      units_per_round=units_per_round,\n",
        "  )\n",
        "  (scm_gen, arm_list, LUCB_bandit, SE_bandit, uniform_bandit) = args\n",
        "\n",
        "  temp_data = scm_gen.generate(10000)\n",
        "  var_hat_tau = []\n",
        "  for arm in arm_list:\n",
        "    arm.reset()\n",
        "    arm_data = arm._data_transformer(temp_data).k_fold(2)\n",
        "    arm._eta.fit(arm_data[0])\n",
        "    var_hat_tau.append(arm.cost * np.var(arm._eta.calculate_score(arm_data[1])))\n",
        "\n",
        "  sorted_vars = np.array(var_hat_tau)\n",
        "  sorted_vars.sort()\n",
        "  min_gap = sorted_vars[1] - sorted_vars[0]\n",
        "\n",
        "  if min_gap \u003e 5:\n",
        "    print(f\"The minimum gap is {min_gap}. Proceeding!\")\n",
        "  else:\n",
        "    print(f\"The minimum gap is {min_gap}. Generating a new instance!\")\n",
        "    continue\n",
        "\n",
        "  for m in range(n_repetitions):\n",
        "    instance_history.append(instance)\n",
        "    all_data = scm_gen.generate(sample_limit + units_per_round * len(arm_list))\n",
        "\n",
        "    # Run LUCB\n",
        "    LUCB_bandit.reset()\n",
        "    np.random.seed(m)\n",
        "    LUCB_results = LUCB_bandit.run(all_data)\n",
        "    LUCB_samples.append(LUCB_results.cum_samples[-1])\n",
        "\n",
        "    # Run SuccessiveElimination\n",
        "    SE_bandit.reset()\n",
        "    np.random.seed(m)\n",
        "    SE_results = SE_bandit.run(all_data)\n",
        "    SE_samples.append(SE_results.cum_samples[-1])\n",
        "    \n",
        "    # Calculate the number of samples needed by uniform_bandit\n",
        "    total_samples_by_arm = [sum(SE_results.samples_by_arm[a]) for a in arm_list]\n",
        "    max_samples = max(total_samples_by_arm)\n",
        "    # uniform_bandit would use max_samples for every arm\n",
        "    uniform_bandit_samples.append(max_samples * len(arm_list))\n",
        "\n",
        "    print((f'instance {instance} had samples: \\tLUCB:{LUCB_samples[-1]}\\t'),\n",
        "        (f'SE:{SE_samples[-1]}\\tuniform:{uniform_bandit_samples[-1]}'))\n",
        "  \n",
        "fig, ax = plt.subplots()#figsize=(7,5))\n",
        "plt.boxplot([LUCB_samples, SE_samples, u_bandit_samples], labels=['CS-LUCB','CS-SE','Uniform'])\n",
        "plt.ticklabel_format(axis='y', style='scientific',scilimits=(0,0))\n",
        "plt.ylim(0, 1.4e5)\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mffJwoYRXo6n"
      },
      "source": [
        "_Expected Output_: The output for this cell should look very similar to the output for Section 5.1, continued. The main difference is that the SCM is non-linear, and non-linear nuisance functions are used. The same level of sample complexity reduction is also present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdrVB14W_NXg"
      },
      "source": [
        "## Section 5.2, continued\n",
        "\n",
        "This cell runs the algorithms for a varying number of ceb.arms. To control the number of arms, we vary $M$, the number of back-door paths; recall that we have $2^M+1$ arms ($2^M$ back-door and 1 frontdoor). For each $M$, run the algorithms over several different instances, then plot the average sample complexity.\n",
        "\n",
        "This cell may take an hour or two to finish running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwQZ8s6U7OfX"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Each element in back_door_size_list is run once\n",
        "back_door_size_list = [3] * 10 + [4] * 10 + [5] * 5 + [6] * 5\n",
        "sample_limit_list = [100000]*20 + [600000]*10\n",
        "\n",
        "LUCB_samples = defaultdict(list)\n",
        "SE_samples = defaultdict(list)\n",
        "uniform_bandit_samples = defaultdict(list)\n",
        "instance_history = []\n",
        "\n",
        "units_per_round = 500\n",
        "\n",
        "instance = -1\n",
        "for (num_back_door, sample_limit) in zip(back_door_size_list, sample_limit_list):\n",
        "  instance +=1\n",
        "  Z_dim = [2] + [3 for _ in range(num_back_door)]  # include Z0\n",
        "  V_dim = [0] + [2 for _ in range(num_back_door)]  # include space for V0\n",
        "  print(f\"At instance {instance} with {num_back_door} back-doors\")\n",
        "  # Approximate the variances to make sure the instance isn't too hard.    \n",
        "  args = ceb.paper_utils.get_section_5_2_example(\n",
        "      Z_dim,\n",
        "      V_dim,\n",
        "      seed=instance,\n",
        "      z_cost=2,\n",
        "      v_cost=1,\n",
        "      z0_cost=5,\n",
        "      num_back_door_paths=3,\n",
        "      sample_limit=sample_limit,\n",
        "      units_per_round=units_per_round,\n",
        "  )\n",
        "\n",
        "  (scm_gen, arm_list, LUCB_bandit, SE_bandit, uniform_bandit) = args\n",
        "  instance_history.append(instance)\n",
        "\n",
        "  np.random.seed(instance)\n",
        "  all_data = scm_gen.generate(sample_limit + units_per_round * len(arm_list))\n",
        "  \n",
        "  np.random.seed(instance)\n",
        "  LUCB_bandit.reset()\n",
        "  LUCB_results = LUCB_bandit.run(all_data)\n",
        "  LUCB_samples[num_back_door].append(LUCB_results.cum_samples[-1])\n",
        "\n",
        "  np.random.seed(instance)\n",
        "  SE_bandit.reset()\n",
        "  SE_results = SE_bandit.run(all_data)\n",
        "  SE_samples[num_back_door].append(SE_results.cum_samples[-1])\n",
        "  \n",
        "  total_samples = max([sum(SE_results.samples_by_arm[a]) for a in arm_list]) * len(arm_list)\n",
        "  uniform_bandit_samples[num_back_door].append(total_samples)\n",
        "        \n",
        "  print((f'instance {instance} with M={num_back_door} had samples: \\tLUCB:{LUCB_results.cum_samples[-1]}\\t'),\n",
        "       (f'SE:{SE_results.cum_samples[-1]}\\tuniform:{total_samples}'))\n",
        "  \n",
        "# box plot code\n",
        "back_door_size = np.array(list(set(back_door_size_list)))\n",
        "num_arms = 1 + 2 ** back_door_size\n",
        "SE_mean_samples = [np.mean(SE_samples[size]) for size in back_door_size]\n",
        "LUCB_mean_samples = [np.mean(LUCB_samples[size]) for size in back_door_size]\n",
        "uniform_mean_samples = [np.mean(uniform_bandit_samples[size]) for size in back_door_size]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "plt.plot(num_arms, SE_mean_samples, label='CS-SE')\n",
        "plt.plot(num_arms, LUCB_mean_samples, label='CS-LUCB')\n",
        "plt.plot(num_arms, uniform_mean_samples, label='Uniform')\n",
        "plt.ylabel(\"Samples\")\n",
        "plt.rc('font', size=20)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNdADDJw1PxS"
      },
      "source": [
        "_Expected Results_: We should see the sample complexity of the uniform algorithm increase about linearly, while the sample complexity of LUCB and SE should increase much more slowly. This is because the sample complexity scales roughly with the sum of the reciprocal squared gaps, which tends to grow much slower than the number of ceb.arms. See Theorem 3 in the paper."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Plot generating code for \"Asymptotically Best Causal Effect Identification with MAB\" .ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1JoSon2_83DhNHax5_TUN4LA1-MQ9xuMW",
          "timestamp": 1642537625851
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
